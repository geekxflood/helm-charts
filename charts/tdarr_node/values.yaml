---
replicaCount: 1

image:
  repository: ghcr.io/haveagitgat/tdarr_node
  pullPolicy: IfNotPresent
  tag: ""
imagePullSecrets: []

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

# Environment variables for Tdarr node
env:
  - name: PUID
    value: "1000"
  - name: PGID
    value: "100"
  - name: TZ
    value: "Europe/Zurich"
  - name: nodeName
    value: "tdarr-node-default"
  - name: serverURL
    value: "http://tdarr-server.media.svc.gxf-cluster:8266"
  - name: serverIP
    value: "tdarr-server.media.svc.gxf-cluster"
  - name: serverPort
    value: "8266"
  - name: transcodegpuWorkers
    value: "4"
  - name: transcodecpuWorkers
    value: "0"
  - name: healthcheckgpuWorkers
    value: "1"
  - name: healthcheckcpuWorkers
    value: "0"
  - name: NVIDIA_VISIBLE_DEVICES
    value: "all"
  - name: NVIDIA_DRIVER_CAPABILITIES
    value: "all"

envFrom: []

# Nodes need GPU runtime
runtime:
  enabled: true
  name: "nvidia"

# Nodes don't need a service (they connect to server)
service:
  enabled: false

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts: []
  tls: []

# GPU resources for transcoding
resources:
  limits:
    nvidia.com/gpu: 1

# Nodes don't expose HTTP endpoints - no probes needed
livenessProbe: {}
readinessProbe: {}

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# NFS cache configuration - each node uses a dedicated subfolder
# Uses shared NFS for transcode cache to avoid iSCSI RWO issues
nfsCache:
  enabled: true
  server: "10.0.0.4"
  path: "/volume1/media/transcode"
  subPath: ""  # Set per instance (e.g., "node-1", "node-2")

# Volume configuration for nodes
# Nodes need:
# - /transcode_cache (NFS) - transcoding cache on shared NFS storage
# - /media/show (PVC: show-pvc, readonly) - TV shows
# - /media/anime (PVC: anime-pvc, readonly) - Anime
# - /media/movie (PVC: movie-pvc, readonly) - Movies
# NOTE: Nodes do NOT need /app/configs - they receive config from server via network
volumes:
  - name: show
    persistentVolumeClaim:
      claimName: show-pvc
  - name: anime
    persistentVolumeClaim:
      claimName: anime-pvc
  - name: movie
    persistentVolumeClaim:
      claimName: movie-pvc

# Additional volumeMounts (cache is handled by template with subPath)
volumeMounts:
  - name: show
    mountPath: /media/show
    readOnly: true
  - name: anime
    mountPath: /media/anime
    readOnly: true
  - name: movie
    mountPath: /media/movie
    readOnly: true

# Node affinity to ensure nodes run on GPU-enabled nodes
nodeSelector: {}
tolerations: []
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.present
          operator: In
          values:
          - "true"
