---
replicaCount: 1

image:
  repository: ghcr.io/haveagitgat/tdarr_node
  pullPolicy: IfNotPresent
  tag: ""
imagePullSecrets: []

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

# Environment variables for Tdarr node
# Override serverURL/serverIP in ApplicationSet for your cluster
env:
  - name: PUID
    value: "1000"
  - name: PGID
    value: "100"
  - name: TZ
    value: "Etc/UTC"
  - name: nodeName
    value: "tdarr-node"
  - name: serverURL
    value: ""  # Set in ApplicationSet (e.g., http://tdarr-server:8266)
  - name: serverIP
    value: ""  # Set in ApplicationSet
  - name: serverPort
    value: "8266"
  - name: transcodegpuWorkers
    value: "4"
  - name: transcodecpuWorkers
    value: "0"
  - name: healthcheckgpuWorkers
    value: "1"
  - name: healthcheckcpuWorkers
    value: "0"
  - name: NVIDIA_VISIBLE_DEVICES
    value: "all"
  - name: NVIDIA_DRIVER_CAPABILITIES
    value: "all"

envFrom: []

# Nodes need GPU runtime
runtime:
  enabled: true
  name: "nvidia"

# Nodes don't need a service (they connect to server)
service:
  enabled: false

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts: []
  tls: []

# GPU resources for transcoding
resources:
  limits:
    nvidia.com/gpu: 1

# Nodes don't expose HTTP endpoints - no probes needed
livenessProbe: {}
readinessProbe: {}

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# NFS cache configuration - each node uses a dedicated subfolder
# Uses shared NFS for transcode cache to avoid RWO issues
nfsCache:
  enabled: false
  server: ""  # Set in ApplicationSet (e.g., "10.0.0.4")
  path: ""    # Set in ApplicationSet (e.g., "/volume1/media/transcode")
  subPath: "" # Set per instance (e.g., "node-1", "node-2")

# Additional volumes (cache is handled by template)
# Override in ApplicationSet for media library PVCs
volumes: []

# Additional volumeMounts (cache is handled by template with subPath)
# Override in ApplicationSet for media library mounts
volumeMounts: []

# Node scheduling - override in ApplicationSet for GPU node affinity
nodeSelector: {}
tolerations: []
affinity: {}
