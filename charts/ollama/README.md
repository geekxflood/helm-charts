# ollama

Ollama - Run large language models locally with GPU support

## Introduction

This chart bootstraps a [ollama](https://github.com/geekxflood/helm-charts/tree/main/charts/ollama) deployment on a [Kubernetes](http://kubernetes.io) cluster using the [Helm](https://helm.sh) package manager.

## Installation

```bash
helm install ollama ./charts/ollama
```

## Configuration

The following table lists the configurable parameters of the ollama chart and their default values.
Please refer to `values.yaml` for the full list of configuration options.

